{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL - LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google LLM - Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatinder/Jatinder/LLMLearning/langchain-tutorials/LearningDocumentation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pipe Operator: | Simple Chain with String Output using ChatPrmoptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear say no to dessert?\\n\\nBecause he was stuffed! üêªüòÇ \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pipe Operator: | Simple Chain with JSON Output using ChatPrmoptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why did the elephant get lost in the ant farm? Because he couldn't find his way out of the anthill!\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic} and give output in JSON only\")\n",
    "\n",
    "jsonOutputParser = JsonOutputParser()\n",
    "\n",
    "json_chain = prompt | model | JsonOutputParser()\n",
    "\n",
    "json_chain.invoke({\"topic\": \"elephant and ant\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coercion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the elephant cross the road? \n",
      "\n",
      "To prove to the donkey that he could! \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That\\'s a pretty good joke! It\\'s funny because it\\'s unexpected and plays on the stereotype of a donkey being stubborn and an elephant being strong. \\n\\nHere\\'s why it works:\\n\\n* **Unexpected Twist:**  The setup leads you to expect a standard \"why did the chicken cross the road\" type of answer, but the punchline is a playful jab at the donkey.\\n* **Animal Stereotypes:** The joke uses the common perception of donkeys being stubborn and elephants being powerful. This makes the punchline funnier because it plays on those familiar ideas.\\n* **Humor in the Absurd:** The idea of an elephant crossing a road just to prove a point to a donkey is inherently absurd, which contributes to the humor.\\n\\nOverall, it\\'s a simple but effective joke that delivers a chuckle! \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "\n",
    "topic = \"elephant and donkey\"\n",
    "\n",
    "#Just to see the result\n",
    "joke_outupt = joke_chain.invoke({\"topic\": topic})\n",
    "print(joke_outupt)\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": joke_chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "composed_chain.invoke({\"topic\": topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other way to use above code. Using Lamdba / inserting custom logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### However, keep in mind that using functions like this may interfere with operations like streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the elephant get lost in the ant farm? \n",
      "\n",
      "Because he couldn't find his way out of the anthill! üêúüêò \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That\\'s a cute joke! It\\'s funny because it plays on the contrast between the large elephant and the tiny ant, and the absurdity of the elephant having to prove anything to the ant.  It\\'s a bit of a silly and unexpected answer, which makes it humorous. \\n\\nHowever, it\\'s not a classic \"laugh-out-loud\" joke. It\\'s more of a chuckle-worthy, lighthearted joke that relies on a bit of wordplay and the unexpectedness of the answer. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "\n",
    "topic = \"elephant and ant\"\n",
    "\n",
    "#Just to see the result\n",
    "joke_outupt = joke_chain.invoke({\"topic\": topic})\n",
    "print(joke_outupt)\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_lamda_chain = joke_chain | (lambda input: {\"joke\": input}) | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "composed_lamda_chain.invoke({\"topic\": topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .pipe() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We could also compose the same sequence using the .pipe() method. Here's what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a decent joke! It\\'s a play on words that uses the double meaning of \"jump\" ‚Äì to quickly enter or exit something, and to travel through hyperspace. \\n\\nHere\\'s why it works:\\n\\n* **Relatable:**  Most people can relate to the frustration of finding a good parking spot. \\n* **Unexpected:** The joke uses a surprising twist by connecting a common issue (parking) with a sci-fi element (Battlestar Galactica).\\n* **Clever:** The pun works because it highlights Starbuck\\'s skill as a pilot, which is relevant to both the joke\\'s setup and punchline.\\n\\n**Overall:**  It\\'s a good, clean joke that will likely get a chuckle from fans of Battlestar Galactica. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "composed_chain_with_pipe = (\n",
    "    RunnableParallel({\"joke\": joke_chain})\n",
    "    .pipe(analysis_prompt)\n",
    "    .pipe(model)\n",
    "    .pipe(StrOutputParser())\n",
    ")\n",
    "\n",
    "composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
